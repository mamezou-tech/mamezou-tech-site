---
title: >-
  Using the Newly Added Tools in the OpenAI API: Remote MCP, Image Generation,
  Code Interpreter
author: noboru-kudo
date: 2025-06-01T00:00:00.000Z
tags:
  - AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
  - OpenAI
  - ç”ŸæˆAI
image: true
translate: true

---

Just last month, OpenAI announced the addition of new built-in tools to the API.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Introducing support for remote MCP servers, image generation, Code Interpreter, and more in the Responses API. <a href="https://t.co/EMZOutvV2a">pic.twitter.com/EMZOutvV2a</a></p>&mdash; OpenAI Developers (@OpenAIDevs) <a href="https://twitter.com/OpenAIDevs/status/1925214114445771050?ref_src=twsrc%5Etfw">May 21, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

In this announcement, the addition of the remote MCP tool in particular has drawn attention. At the same time, OpenAI also announced joining the MCP Steering Committee, giving the impression that OpenAIâ€”previously behind other platformsâ€”is finally beginning full-scale support for the MCP ecosystem.

Here, I investigated how to use the newly added tools, including MCP, and will introduce them below.

- [Remote MCP](#remote-mcp)
- [Image Generation](#image-generation)
- [Code Interpreter](#code-interpreter)

## Remote MCP

[OpenAI Doc - Tools - Remote MCP](https://platform.openai.com/docs/guides/tools-remote-mcp)

The Remote MCP tool is a new feature that allows you to use tools hosted on external MCP servers via the Responses API. Since OpenAI handles everything from discovering available tools to selecting the most appropriate one and executing it, developers don't need to implement any tool execution logic.

This time, I tested using Devin's [DeepWiki](https://docs.devin.ai/work-with-devin/deepwiki-mcp) MCP server, which is introduced in the official documentation.

I will implement an example where it summarizes the prerequisites and usage of OpenAI's [Codex CLI](https://github.com/openai/codex).

```python
from openai import OpenAI

client = OpenAI()

response = client.responses.create(
    model='gpt-4.1-mini',
    input='ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã¨åˆ©ç”¨æ‰‹é †ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§200æ–‡å­—ç¨‹åº¦ã§ã¾ã¨ã‚ã¦ã€‚ GitHub Repository: openai/codex',
    # Specify MCP tool
    tools=[{
        'type': 'mcp',
        'server_label': 'deepwiki',
        'server_url': 'https://mcp.deepwiki.com/mcp',
        # If executing without approval
        # 'require_approval': 'never',
    }],
)

while any(entity.type == 'mcp_approval_request' for entity in response.output):
    # Approval process
    approval_inputs = []
    for entity in response.output:
        if entity.type == 'mcp_approval_request':
            print((
                '*' * 10 + ' Executing Tool ' + '*' * 10 + '\n'
                f'Request ID: {entity.id}\n'
                f'Tool: {entity.name}\n'
                f'Arguments: {entity.arguments}\n'
                f'Label: {entity.server_label}\n'
            ))
            approval_inputs.append({
                'type': 'mcp_approval_response',
                'approval_request_id': entity.id,
                'approve': True
            })
    # Approval & MCP tool execution
    response = client.responses.create(
        model='gpt-4.1-mini',
        previous_response_id=response.id, # Maintain context
        tools=[{
            'type': 'mcp',
            'server_label': 'deepwiki',
            'server_url': 'https://mcp.deepwiki.com/mcp',
        }],
        input=approval_inputs
    )

print('*' * 10 + ' Final Execution Result ' + '*' * 10)
print(response.output_text)
```

What stands out in the above code is the approval process before executing the MCP tool.

If you set `require_approval` to `never`, you can skip this approval process, but from a security perspective, it's recommended to include the approval step unless the MCP server is fully trusted.

At runtime, the Responses API provides details of the tool to be used and its arguments as `mcp_approval_request`. On the client side, you review that information and, if everything is fine, approve it with `mcp_approval_response`.

In this test, I implemented it to output the tool details and auto-approve them.

When I ran this code, I got the following result:

```
********** å®Ÿè¡Œãƒ„ãƒ¼ãƒ« **********
ãƒªã‚¯ã‚¨ã‚¹ãƒˆID: mcpr_6836d85f88108191af93f624edf62e83032c59875e6c1154
ãƒ„ãƒ¼ãƒ«: read_wiki_structure
å¼•æ•°: {"repoName":"openai/codex"}
ãƒ©ãƒ™ãƒ«: deepwiki

********** å®Ÿè¡Œãƒ„ãƒ¼ãƒ« **********
ãƒªã‚¯ã‚¨ã‚¹ãƒˆID: mcpr_6836d86e63d0819180d4b1ca6b839828032c59875e6c1154
ãƒ„ãƒ¼ãƒ«: read_wiki_contents
å¼•æ•°: {"repoName":"openai/codex"}
ãƒ©ãƒ™ãƒ«: deepwiki

********** æœ€çµ‚å®Ÿè¡Œçµæœ **********
ä»¥ä¸‹ã¯OpenAI Codex CLIã®ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã¨åˆ©ç”¨æ‰‹é †ã®æ¦‚è¦ã§ã™ã€‚

---

## ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶
- å¯¾å¿œOS: macOS 12ä»¥ä¸Šã€Ubuntu 20.04ä»¥ä¸Šã€Debian 10ä»¥ä¸Šã€Windows 11ï¼ˆWSL2çµŒç”±ï¼‰
- Node.js 22ä»¥ä¸Šï¼ˆLTSæ¨å¥¨ï¼‰
- Git 2.23ä»¥ä¸Šï¼ˆPRãƒ˜ãƒ«ãƒ‘ãƒ¼åˆ©ç”¨æ™‚ï¼‰
- ãƒ¡ãƒ¢ãƒª: æœ€ä½4GBï¼ˆæ¨å¥¨8GBï¼‰

## åˆ©ç”¨æ‰‹é †
1. Codex CLIã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€OpenAI APIã‚­ãƒ¼ã‚’è¨­å®š
2. ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ `codex` ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—è‡ªç„¶è¨€èªã§æ“ä½œ
3. ã‚³ãƒ¼ãƒ‰ã®è§£æãƒ»ä¿®æ­£ã€ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã¯æ‰¿èªãƒ¢ãƒ¼ãƒ‰ã«å¾“ã„å‹•ä½œ
4. ãƒ¢ãƒ¼ãƒ‰ã¯ã€ŒSuggestï¼ˆææ¡ˆï¼‰ã€ã€ŒAuto Editï¼ˆè‡ªå‹•ç·¨é›†ï¼‰ã€ã€ŒFull Autoï¼ˆå®Œå…¨è‡ªå‹•ï¼‰ã€ã‹ã‚‰é¸æŠå¯èƒ½
5. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãŸã‚ã‚³ãƒãƒ³ãƒ‰ã¯sandboxç’°å¢ƒã§å®Ÿè¡Œã•ã‚Œã‚‹ï¼ˆmacOSã¯Apple Seatbeltç­‰ï¼‰

---

å¿…è¦ã«å¿œã˜ã¦APIã‚­ãƒ¼è¨­å®šã‚„ã‚³ãƒ³ãƒ•ã‚£ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨ã„ã¦è©³ç´°è¨­å®šã‚‚å¯èƒ½ã§ã™ã€‚
```

The MCP tool was executed twice (`read_wiki_structure` and `read_wiki_contents`), and the response was generated based on each result. You can clearly see how the LLM autonomously decides which tools to use and executes them.

I summarized the execution flow of Remote MCP that I understood from this test in the diagram below.

```mermaid
sequenceDiagram
    participant C as Client
    participant R as OpenAI<br>Responses API
    participant S as MCP Server
    participant LLM
    C ->> R: å®Ÿè¡Œ (with MCPãƒ„ãƒ¼ãƒ«)
    R ->> S: åˆ©ç”¨å¯èƒ½ãƒ„ãƒ¼ãƒ«å–å¾—
    R ->> R: å‡ºåŠ›ç”Ÿæˆ(mcp_list_tools)
    R ->> LLM: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ(with MCPåˆ©ç”¨å¯èƒ½ãƒ„ãƒ¼ãƒ«)
    LLM -->> R: MCPåˆ©ç”¨ãƒ„ãƒ¼ãƒ«åãƒ»å¼•æ•°
    R -->> C: ãƒ„ãƒ¼ãƒ«æ‰¿èªä¾é ¼(mcp_approval_request)
    loop æ‰¿èªãƒªã‚¯ã‚¨ã‚¹ãƒˆ(mcp_approval_request)ãŒå­˜åœ¨ã™ã‚‹
        C ->> C: æ‰¿èªãƒ—ãƒ­ã‚»ã‚¹
        C ->> R: æ‰¿èª(mcp_approval_response)
        R ->> S: MCPãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ(HTTP)
        R ->> LLM: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ(with MCPãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœ)
        LLM ->> LLM: MCPãƒ„ãƒ¼ãƒ«å®Ÿè¡Œåˆ¤æ–­
        alt MCPãƒ„ãƒ¼ãƒ«å®Ÿè¡Œè¦
            LLM -->> R: MCPåˆ©ç”¨ãƒ„ãƒ¼ãƒ«åãƒ»å¼•æ•°
            R -->> C: ãƒ„ãƒ¼ãƒ«æ‰¿èªä¾é ¼(mcp_approval_request)
        else
            LLM -->> R: ãƒ†ã‚­ã‚¹ãƒˆ
            R -->> C: æœ€çµ‚çµæœ
        end
    end
```

Integration between external MCP ecosystems and the OpenAI API appears set to become significantly easier.

## Image Generation

[OpenAI Doc - Tools - Image generation](https://platform.openai.com/docs/guides/tools-image-generation)

OpenAI has long had a dedicated [Image API](https://platform.openai.com/docs/api-reference/images), but now image generation has also been added as a tool in the Responses API.

Using it as a tool in the Responses API enables multi-turn image generation and streaming support, unlike the traditional one-shot generation. This should allow you to build more flexible and interactive image generation workflows.

Of course, the existing Image API will continue to be supported, so for simple image generation, you can still choose the traditional method. For guidance on when to use which API, refer to the [official guide](https://platform.openai.com/docs/guides/image-generation).

### New Generation

First, let's try basic image generation. Here is an example that creates a banner for a fictional event.

```python
from openai import OpenAI
import base64

client = OpenAI()

response = client.responses.create(
    model='gpt-4.1-mini',
    input=(
        'è±†è”µãƒ‡ãƒ™ãƒ­ãƒƒãƒ‘ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆã€ŒMamezou Tech Fest 2025ã€ã®å‘ŠçŸ¥ãƒãƒŠãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n'
        '- æ˜ã‚‹ãæ¥½ã—ã„é›°å›²æ°—\n'
        '- è±†ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŸã¡ãŒé›†ã¾ã£ã¦ã„ã‚‹æ§˜å­\n'
        '- ã‚¤ãƒ™ãƒ³ãƒˆåã¨æ—¥ä»˜ã€Œ2025å¹´7æœˆ20æ—¥ã€å…¥ã‚Š\n'
        '- ã‚¢ãƒ‹ãƒ¡ã‚¹ã‚¿ã‚¤ãƒ«ã€ã‚«ãƒ©ãƒ•ãƒ«ãªé…è‰²\n'
    ),
    # Specify image generation tool
    tools=[{
        'type': 'image_generation',
        'size': '1024x1536',
        'quality': 'medium',
        'output_format': 'webp',
        'model': 'gpt-image-1', # Responses API currently supports only this (DALL-E is not available)
    }],
    tool_choice={'type': 'image_generation'}
)

images = [
    output.result for output in response.output
    if output.type == 'image_generation_call'
]

if images:
    with open('banner.webp', 'wb') as f:
        f.write(base64.b64decode(images[0]))
```

The tool parameters allow fine-grained settings such as size, quality, and output format. For details on available parameters, see the official documentation below.

- [OpenAI Doc - Image Generation - Customize Image Output](https://platform.openai.com/docs/guides/image-generation#customize-image-output)

When I ran this, the following image was generated.

<img alt="banner" src="https://i.gyazo.com/0499c5e3ee3fba206408db6becca89f9.webp" width="200" height="300" style="margin:10px">

Whether good or bad, you get a plausible banner image (of course, the event is fictional).

### Editing

Next, I tested the image editing feature using our companyâ€™s mascot (?), Mameka, in a singer version.

Here is the original image.

<img alt="original image" src="/img/logo/mameka4.png" width="150" height="150" style="margin:10px">

The sample code looks like this:

```python
from openai import OpenAI
import base64

client = OpenAI()

# Original image
with open("mameka.png", "rb") as image_file:
    base64_image = base64.b64encode(image_file.read()).decode('utf-8')

response = client.responses.create(
    model='gpt-4.1-mini',
    input=[{
        'role': 'user',
        'content': [{
            'type': 'input_text',
            'text': 'ç¬‘é¡”ã§æ­Œã£ã¦ã„ã‚‹ã‚ˆã†ã«ç·¨é›†ã—ã¦ãã ã•ã„'
        },
        # Image to edit
        {
            'type': 'input_image',
            'image_url': 'data:image/png;base64,' + base64_image
        }],
    }],
    tools=[{'type': 'image_generation'}],
    tool_choice={'type': 'image_generation'}
)

images = [
    output.result for output in response.output
    if output.type == 'image_generation_call'
]

if images:
    with open('singer-mameka.webp', 'wb') as f:
        f.write(base64.b64decode(images[0]))
```

The image to edit is passed to the Responses API together with the prompt. In this example, I embedded the image data in Base64, but you can also upload the file using the [File API](https://platform.openai.com/docs/api-reference/files).

Here is the result of the image editing.

<img alt="singer-mameka" src="https://i.gyazo.com/82f3fbd0ef0e03a16bc81e5a944c95f1.png" width="150" height="150" style="margin:10px">

The original characteristics of the image are preserved, and the expression has been naturally changed to look like singing. Itâ€™s quite a wonderful resultğŸ’–

Although I didnâ€™t try it here, you can also pass a masked original image and overwrite only that part using inpainting[^1].

[^1]: <https://platform.openai.com/docs/guides/image-generation?image-generation-model=gpt-image-1&api=responses#edit-an-image-using-a-mask-inpainting>

## Code Interpreter

[OpenAI Doc - Tools - Code Interpreter](https://platform.openai.com/docs/guides/tools-code-interpreter)

Finally, letâ€™s try the Code Interpreter feature. This feature was already available in the Assistants API, but it is now also accessible via the Responses API.

In the Responses API, the Code Interpreter introduces a new concept called a container. This represents a sandboxed Python execution environment and is used slightly differently than in the Assistants API.

This time, I tested an example where we analyze a sample CSV file and generate a graph. Since the code is a bit long, I will explain it in two parts.

### Creating a Container and Calling the Responses API

First, here is the part where we use the Code Interpreter to call the Responses API.

```python
from openai import OpenAI

client = OpenAI()

# CSV file to analyze
with open('sales.csv', 'rb') as data_file:
    file_response = client.files.create(
        file=data_file,
        purpose='user_data'
    )
# Upload Japanese font
# https://fonts.google.com/share?selection.family=Noto+Sans+JP:wght@100..900
with open('NotoSansJP-Regular.ttf', 'rb') as font_file:
    font_response = client.files.create(
        file=font_file,
        purpose='user_data'
    )

# Create container
container = client.containers.create(
    name='sales_data', file_ids=[file_response.id, font_response.id]
)

response = client.responses.create(
    model='gpt-4.1-mini',
    input=[{
        'role': 'user',
        'content': [{
            'type': 'input_text',
            'text': 'ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å£²ä¸Šæ§‹æˆæ¯”ï¼ˆå††ã‚°ãƒ©ãƒ•ï¼‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ãƒ•ã‚©ãƒ³ãƒˆã¯Noto Sans JPã‚’é©ç”¨ã—ã¦ãã ã•ã„ã€‚'
        }],
    }],
    # Specify code interpreter tool
    tools=[{
        'type': 'code_interpreter',
        'container': container.id,
    }],
    tool_choice={'type': 'code_interpreter'}
)
```

First, the files needed for analysis are uploaded via the [File API](https://platform.openai.com/docs/api-reference/files). In this example, we prepare a CSV file for analysis and a font file so that Japanese can be used in the graph.

Next, we create a container object using the newly added [Containers API](https://platform.openai.com/docs/api-reference/containers). By specifying the IDs of the uploaded files here, they are mounted into the sandbox environment.

Note that creating a container currently incurs a cost of $0.03 each. For up-to-date pricing, please check the [official pricing information](https://platform.openai.com/docs/pricing).

Finally, we specify the Code Interpreter tool and call the Responses API, passing the container ID in the tool parameters (`container`).

:::column:Automatically Creating Containers
In addition to manually creating containers, you can also have them generated automatically when calling the Responses API.

In this case, the files to mount in the container are specified as tool parameters when calling the Responses API.

```python
response = client.responses.create(
    model='gpt-4.1-mini',
    input=[{
        # (omitted)
    }],
    tools=[{
        'type': 'code_interpreter',
        # If automatically creating the container
        'container': {
            'type': 'auto',
            'file_ids': [file_response.id, font_response.id],
        }
    }],
    tool_choice={'type': 'code_interpreter'}
)
```
:::

### Retrieving the Output File

Next, here is the second part of the source code. Here, we extract the Code Interpreterâ€™s output file from the Responses API response.

```python
annotations = [
    annotation for annotation in response.output[-1].content[0].annotations
    if annotation.type == 'container_file_citation'
]

if annotations:
    output_response = client.containers.files.content.with_raw_response.retrieve(
        container_id=annotations[0].container_id,
        file_id=annotations[0].file_id,
    )
    if output_response.status_code == 200:
        with open('sales_summary.png', 'wb') as f:
            f.write(output_response.content)
    else:
        print(
            f'Error retrieving file: {output_response.status_code} - {output_response.text}')
```

The files generated by the Code Interpreter must be retrieved from the container.

From the Responses API output `annotations`, extract the file ID and container ID from the `container_file_citation`. Then use the newly introduced [Containers File API](https://platform.openai.com/docs/api-reference/container-files/retrieveContainerFileContent) to retrieve the actual file data.

### Sample File and Execution Result

Finally, letâ€™s run the Code Interpreter.

For this test, I prepared the following sample CSV file.

**CSV file (sales.csv)**
```csv
æ³¨æ–‡ID,æ³¨æ–‡æ—¥,å•†å“å,ã‚«ãƒ†ã‚´ãƒª,æ•°é‡,å˜ä¾¡ï¼ˆå††ï¼‰
1001,2025-05-01,ã‚Šã‚“ã”,æœç‰©,10,120
1002,2025-05-01,ãƒãƒŠãƒŠ,æœç‰©,8,90
1003,2025-05-02,ãƒ¡ãƒ­ãƒ³,æœç‰©,1,800
1004,2025-05-02,ã‚­ãƒ£ãƒ™ãƒ„,é‡èœ,5,150
1005,2025-05-03,ç‰›ä¹³,é£²æ–™,3,180
1006,2025-05-04,ãƒˆãƒãƒˆ,é‡èœ,6,130
1007,2025-05-04,ã‚ªãƒ¬ãƒ³ã‚¸,æœç‰©,7,100
1008,2025-05-05,ãƒ¨ãƒ¼ã‚°ãƒ«ãƒˆ,ä¹³è£½å“,4,200
1009,2025-05-06,ã«ã‚“ã˜ã‚“,é‡èœ,10,80
1010,2025-05-06,ã‚³ãƒ¼ãƒ’ãƒ¼,é£²æ–™,2,250
```

When executed, the following graph was generated.

<img alt="code interpreter output file" src="https://i.gyazo.com/9d00693414469d598b75a4a997f45df3.png" width="400" height="400" style="margin:10px">

The Japanese font was applied correctly, and a clear pie chart showing the composition ratio by category was created.

In this test, I only verified CSV file analysis, but like ChatGPT, it seems capable of handling various file formats and data processing tasks.

## Summary

In this article, I tested the various tool features newly added to the Responses API.

These features are valuable on their own, but combining them could enable even more powerful AI agent constructions.

Indeed, OpenAIâ€™s [Agents SDK](https://openai.github.io/openai-agents-python/tools/) already supports these tools, and I feel the AI agent development ecosystem is rapidly evolving.

Through this test, I realized that the Responses API has entered a new stage of AI utilization. Iâ€™m very excited to see what use cases will emerge next.
