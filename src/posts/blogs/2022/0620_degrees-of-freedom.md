---
title: 統計学で避けて通れない自由度の話
author: shuichi-takatsu
date: 2022-06-20
tags: [Analytics]
---

”自由度”…統計学を勉強したことがある人なら、誰でも一度は耳にしたことのある言葉でしょう。  
”自由度”を教科書通りに説明すると「自由に決めることができる値の個数」ですが、何度聞いても何度考えてもピンときません。  
統計学を勉強していて”自由度”で躓いた人も多いのではないでしょうか。    
今回はそんな”自由度”についてお話したいと思います。

[[TOC]]

## 平均とは

あるデータの母集団があり、そこからサンプルとしてa, b, cの３つのデータを抽出したとしましょう。
簡単に考えるために

> a = 1  
> b = 2  
> c = 3

とします。
a, b, cを変数${x_i}$とすると平均${\bar{x}}$は  

$\bar{x} = {\sum_{i=1}^{n} {x_i} \over n}$

で計算できます。ｎはサンプルの個数です。  
サンプル数が3であることから、$\bar{x}$ = 2 となります。  
これを「標本(サンプル)平均」と言います。a, b, cの各データが母集団からのサンプルだからです。

## 自由度って何？

統計の教科書を読むと、おそらく自由度の説明は  

`自由に決めることができる値の個数`

のように書かれているものが多いと思います。  

統計学で推定・検定を行う対象の多くは母集団の「平均」や「分散」です。  
推計統計学では人間は「平均」や「分散」などの母数(パラメータ)を正確に知ることが出来ないので、母数を神のみぞ知る”定数”として扱います。  
平均は”定数”として決まっているので、母集団から得られるデータがどんな値を取ることが出来るのかを考えていくことになります。  
（そろそろ思考が追い付かないですよね。私はこのあたりで”なぜ？”って思いました）  

`平均(パラメータ) ＝ 定数`

としたときに、データ（変数）a, b, cのうちの２個までは自由に値を設定できますが、最後の１つ（３番目）は、平均の式から逆算して値が”固定”されてしまうという理屈から「自由な値を取り得ない」として定数扱いされてしまいます。  
つまり「最後の１つは自由に値を決めることが出来ない」＝「自由度が1つ減る」と考えて、自由度は

`3(データ個数) - 1(固定値の数) = 2(自由度)`

となります。  
大抵の教科書の説明はここまで。  
しかし今の話を聞いても私にはまったく腑に落ちないのです。  

標本平均はa, b, cの３つの値をサンプリングした時点で初めて計算されたわけで、平均を出す前に値が決定しているような理屈は”因果が逆転”しているように感じられました。  
神のみぞ知る真の平均は標本の平均とは異なるだろう、と考えたからです。

## もう一つの解釈

他の説明も見つけました。それは

`観察値の数から推計値を除いた数`

のような説明でした。  
観察値の数というのは上記の例で言えばa, b, cの3つです。  
このa, b, cから計算された「平均」という値は”推計値”です。  
観察値の数”３”から、推計値である平均の値の数”１”を引くとそれが”自由度”であると考えます。

これは単に”自由度というものをそのように設定した”というだけで、ここまで聞いても腑に落ちません。

## 分散を使ってもう少し筋を通す

自由度を使う場面は分散の計算でお目にかかります。  
分散を使って調べてみましょう。

あるデータの母集団からサンプリングされたデータ数ｎの”標本平均”の計算は

$\bar{x} = {\sum_{i=1}^{n} {x_i} \over n}$

です。これはいままで通り。  
データのばらつき(分散)を考えるときは、偏差の和は

${\sum_{i=1}^{n} ({x_i} - \bar{x}) = 0}$

になってしまうので平方和で考えます。  

${\sum_{i=1}^{n} ({x_i} - \bar{x})^2 = S}$(平方和)

分散を計算するときに平方和を単純にｎで割ってしまう困ったことが起きます。  
それはｎ個のデータの平方和の計算にも$\bar{x}$が利用されているので、実際に自由な値が取れる数はｎ－１個であり、平方和をｎで割ってしまうと本来（神のみぞ知る）の分散の値よりも小さくなってしまいます。  
この「ｎ－1」という値が自由度になります。

${\sum_{i=1}^{n} ({x_i} - \bar{x})^2 \over n - 1}{ = σ^2}$(分散)

分散を登場させることで、最初の２つの説明よりは少し理解しやすくなりました。  
割る数をｎからｎ－１としてあげることで、実際の分散の値に”近づく”という理屈です。  
しかし、やっぱりモヤモヤします。  
自由度がｎ－１だとしても、平方和の計算にはｎ個の偏差が使われているのに、割る数がｎではなくｎ－１の方が妥当だという”感覚”が持てないのです。

## 誤差という考え方

筆者は学生の頃、実は数学が苦手でした。苦手というよりも”なぜ？”と思うことが多くて好きになれなかったのです。  
線形代数や積分・微分…どれも数式としては美しいと思います。  
計算をすれば答えは出ますが、”で？それで何？”と思うことが多く、本気で取り組む気力が起きませんでした。

その代わり物理や化学は大好きでした。  
扱う”対象”がはっきりわかっていたので、速度を”微分”したものが加速度だと言われれば素直に納得できましたし、計算していてスッと頭に入ってきたものです。

”自由度”にも”具体的な数式としての説明”が欲しいと感じていました。

そこで次は誤差という考え方を導入しましょう。  
先にネタバラシをしてしまうと、この考え方は次の書籍で勉強しました。  

[ソフトウェアメトリクス統計分析入門―現場エンジニアによる直観的解説と実践ドリル](https://www.amazon.co.jp/dp/4817195584)

先ほどa, b, cというサンプルを考えて平均を計算しました。 
計算した平均は”標本平均”です。   
母集団の真の平均μは神のみぞ知る数値なので我々には分かっていない設定です。  
よって、サンプルから計算された平均$\bar{x}$と真の平均μとの間には必ず”誤差”が含まれます。

この誤差のために真の平方和(書籍では変動と説明されています)とサンプルから計算された平方和では必ず誤差分の差がでます。  
平方和の性質上、$\bar{x}$が真の平均μより大きくても小さくても、サンプルから計算された平方和は”必ず”真の平方和よりも小さくなります。

例として、サンプルが以下のようだったとします。
> a = 1  
> b = 2  
> c = 3

標本平均$\bar{x}$は2になります。  
その場合の平方和は

${\sum_{i=1}^{n} ({x_i} - 2)^2 = 2}$ (平方和)

となります。  
しかし、真の平均が1.9だった場合の平方和は

${\sum_{i=1}^{n} ({x_i} - 1.9)^2 = 2.03}$ (平方和)

となります。  
また真の平均が2.1だった場合の平方和は

${\sum_{i=1}^{n} ({x_i} - 2.1)^2 = 2.03}$ (平方和)

となり、標本平均を使った平方和は必ず真の平方和よりも値が小さくなってしまいます。  

まだ見ぬ分散を求めるには上記で言うところの”誤差分”を平方和に”予め”加算しておかなければなりません。  
ここで言う”誤差”は平均$\bar{x}$の分布の分散になるので、誤差は元の分散${σ^2}$の$1 \over n$です。

よって標本平均$\bar{x}$のそれぞれに${σ^2 \over n}$の誤差があると考えます。  

平方和の計算にｎ個の標本平均$\bar{x}$が使用されているので、合計誤差として${σ^2 \over n}{\times n}$つまり$σ^2$が付加されます。

求める分散の式としては  

${\sum_{i=1}^{n} ({x_i} - \bar{x})^2} + σ^2 \over n$ = $σ^2$

になります。これを順次展開していきます。  
全体にｎをかけます。  

${\sum_{i=1}^{n} ({x_i} - \bar{x})^2} + σ^2 = n \times σ^2$  

両辺に求めるべき分散が来ているので、分散を右辺に集め  

${\sum_{i=1}^{n} ({x_i} - \bar{x})^2} = (n - 1) \times σ^2$  

最終的にｎ－１で平方和を割る式となりました。  

${\sum_{i=1}^{n} ({x_i} - \bar{x})^2} \over n - 1$ = $σ^2$(分散)

真の平均からのずれ（誤差）を考慮して標本データの平方和に誤差分を上乗せすることで、分散を求める数式において平方和を自由度”ｎ－１”で割る理屈が理解できました。

## まとめ

統計の勉強をしていると”自由度の意味”が引っかかり頭の片隅にモヤモヤしたものが残っていましたが、誤差というものを考慮することで、平方和を自由度ｎ－１で割る理由が「数式として」導かれました。  

なぜ自由度という概念が存在するかを深く追求しなくても統計の計算は出来ますが、理屈を分かっておいた方が今後の理解の助けになると思います。

もちろん、この展開公式はサンプルを用いて母集団を推定する推計統計ならではの理論展開です。  
記述統計のように”目の前に見えているデータがすべて”という統計なら、素直にｎで割るべきです。そこにあるデータが母集団そのものなのですから。

[統計解析ツール紹介やその活用方法をまとめています。](https://developer.mamezou-tech.com/analytics/)

データ分析に活用して頂ければ幸いです。
