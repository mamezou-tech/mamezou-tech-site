---
title: ChatGPTのベースになった自然言語処理モデル「Transformer」を調べていたら「HuggingFace」に行き着いた
author: shuichi-takatsu
date: 2023-03-20
tags: [ディープラーニング, 深層学習, 機械学習, chatgpt, transformer, huggingface]
---

## ChatGPT を使ってみる

最近「[ChatGPT](https://chat.openai.com/)」というワードがSNSを賑わしているようです。  
筆者は技術に対しては非常にミーハー(？)なので、ChatGPTが何なのか？また、どれほど凄いのか試してみたくなりました。  
さっそく ChatGPT を使ってみたいと思います。  
(ChatGPTの利用登録の方法については、解説サイトがたくさんありますのでそちらを参照ください。Googleアカウントを持っていればGoogleアカウントと連携してログインすることも可能です)  

ChatGPTに登録してログインすると以下のような画面が表示されます。  

![](https://gyazo.com/c10d7b16abd647d8d500d849851a4638.png)

Examplesを使って、いくつかの問いかけに対する応答を確認することが出来ます。  
試しにExamplesの一番上にある「Explain quantum computing in simple terms」を選択して ChatGPT へ問いかけをしてみます。  
(問いかけの内容は「量子コンピュータについて」のようです)  

問いかけを実行すると以下のような応答が帰ってきました。  
![](https://gyazo.com/841dd7a5f9c394808f6d156988c29fcc.png)

応答文はかなりの長文です。  
問いかけの内容が英語だと分かりにくいので(単に筆者の英語力が足りないのですが)、日本語で問いかけてみます。  
![](https://gyazo.com/9b1b9371d353c0ba5949c5320c532df9.png)

なんと！  
とても自然な日本語で応答が帰ってきました。  
出力された文章を読んでもまったく違和感がありません。  
Google検索の結果とは違って「どこかで公開されている文書へのリンク」ではなく、いま作成された日本語の文章です。  
まるでチャットの向こう側に普通に人間が居て対応しているかのような自然な文章です。  

ChatGPT に英語で問い合わせると英語で、中国語で問い合わせると中国語で応答がありました。  

次に、ベタな展開ですが ChatGPT に「ChatGPTとは？」と問いかけてみます。  
以下のような応答が帰ってきました。  
![](https://gyazo.com/1c303bc05baa145aa3621e05febb268c.png)

応答文の中に「GPT-3.5アーキテクチャ」という聞き慣れない単語が出てきたので、さらに問いかけてみます。  
以下のような応答が帰ってきました。  
![](https://gyazo.com/6fbdb3d80c6d22918587eb2cb74e9dd5.png)

応答文の中に「トランスフォーマー」という単語が入っています。  
「トランスフォーマー」という”カタカナ文字”を見てしまうと、日本でもヒットしたハリウッド映画を連想してしまいそうですが、ここで言っている「トランスフォーマー」は深層学習モデルの「Transformer」のことです。  

試しに、少しマイナーな話題を ChatGPT に問いかけてみましょう。  
問いかけの内容は、筆者が昔からプレイしているゲームについてです。  
![](https://gyazo.com/62e0d717e306e2d5429bbf7b59e18c11.png)

ゲーム自体は2014年にリリースされた古参のゲームです。  
今ではそれほどユーザ数も多くないと思われるゲームなのですが、しっかりと応答してくれています。  
ChatGPT が必ず正解を答えてくれるとは限りませんが、想像していた以上に流暢な日本語で応答することに驚愕です。  
他にも色々な質問や感想を尋ねてみましたが、どれもスムーズな会話になりました。  
![](https://gyazo.com/f0625b9954a8e9bb6108bc881c00ad61.png)

このまま ChatGPT と会話をしているだけで、投稿ネタが出来てしまうんじゃないか？と思うほど自然な日本語で応答が帰ってきます。  

## トランスフォーマーとは

応答文の中にあった「トランスフォーマー(Transformer)」について調べてみます。

トランスフォーマー（Transformer）はGoogleが開発した自然言語処理のための深層学習モデルの一つです。  
トランスフォーマーが登場する前の自然言語処理の主流は「再帰型ニューラルネットワーク(RNN)」でしたが、RNNはその名の通り「再帰型」なので、再帰を繰り返すことで学習に必要な「勾配」が消失してしまうなどの欠点がありました。  
また長い文章を処理すると爆発的に計算量が増えてしまい処理できなくなることもあります。  

それに対してトランスフォーマーは対象全体を一度に処理することが出来る「Attention」という仕組みを採用しており、RNNよりも効率的に処理することが出来ます。  

[この記事](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)(2017年8月31日)で Transformer が紹介されていて、さらに「[Attention Is All You Need](https://arxiv.org/abs/1706.03762)」というタイトルの記事が紹介されています。  
(このタイトルもちょっぴりハリウッド映画を想像してしまいますね)  

「Attention」の英単語の意味は「注意」です。  
人間は文章を読むときにすべての文言を均等に意識して読んでいるわけではありません。文章の中の重要な部分に「注意」を払いつつ文章を読んでいるはずです。  
Attention は、文章や時系列データなどの連続データの中の「どこに注目すべきか」を機械学習的に特定する仕組みです。  

## Attension の何がすごいのか

我々の生活に関連する様々な分野で機械学習／深層学習が有効に働くことは既に実証されてきましたが、学習が高度になればなるほど「なぜその結果に辿り着いたのか？」を説明することが困難になってきていました。  
つまり「推論結果は正しいと思われるが、なぜその推論結果を導き出したのか」を客観的に説明することが困難でした。  

例えば[以前の投稿](/blogs/2023/01/14/openvino-02/)で、ある画像を画像認識モデルに入力して、その画像に写っているものは「猫」である可能性が高いと画像認識モデルは推論しました。  
しかし、画像認識モデルのどの層がどういう働きをして画像の中に写っているものが猫だと認識するに至ったのかという「判断根拠」を説明することは困難なことでした。(最近は判断根拠の可視化と説明可能性の研究がかなり進んでいると思いますが)  

Attention は学習した結果として「どの部分に注意したか」を示すことが出来るので「判断根拠」の可視化という意味で非常に有効な手段になります。  
「判断根拠」が可視化されてその根拠に納得できるようになれば、色々な意思決定に役立つと思います。  

## Transformer の系譜

RNN以降のTransformerの系譜を少し調べました。  
概略を以下に示します。  
![](https://gyazo.com/c9a6d07010a3bf58a0482858891f297d.png)

Transformerが2017年に発表されて以降、非常に早いペースで開発が進んでいることがわかります。
GPT-1に続き、GPT-2、GPT-3、GPT-4と次々に新しいトランスフォーマー技術が開発されています。  
最新の GPT-4 は2023年3月14日に公開されたようです。

ミーハーな筆者としては「とにかく簡単にトランスフォーマーを試してみたい」ということで、色々と調べました。  
すると [HuggingFace](https://huggingface.co/) というサイトを見つけました。

## HuggingFace を使って事前学習済みモデルで遊ぶ

折角なので「HuggingFace」について ChatGPT に問い合わせてみましょう。  
以下のような応答がありました。  
> 「Hugging Faceは、自然言語処理（NLP）の分野で活躍する人々のためのオープンソースのライブラリやツールを提供している企業です。Hugging Faceは、トランスフォーマーなどのニューラルネットワークモデルを中心に、NLPの最新の研究成果を共有し、開発者がより簡単にNLPアプリケーションを構築できるように支援しています。Hugging Faceの最も有名な製品は、Transformersと呼ばれるオープンソースのライブラリです。Transformersは、多くの事前学習済み言語モデルを提供し、これらを特定のタスクに微調整することで、高度なNLPアプリケーションを構築できるようになっています。また、Hugging Faceは、多言語対応の機械翻訳、感情分析、テキスト生成など、さまざまなNLPタスクをカバーするライブラリも提供しています。」

HuggingFace では様々な事前学習済みモデルを公開しているようです。  

さっそく [HuggingFace](https://huggingface.co/) に登録して使用してみます。  
登録画面で「メールアドレス」と「パスワード」を登録すると、確認メールが送信されてくるので確認を実行して登録が完了します。  
ログインすると以下のような画面が表示されました。  
![](https://gyazo.com/23225c5a924d8143de0d7cfa3207994b.png)

HuggingFace サイトの中の「[models](https://huggingface.co/models)」を選択します。  
![](https://gyazo.com/dc185998880726b88bab8428406cfefc.png)

様々な種類の機械学習モデルが用意されているようです。  
今回は文章を生成してみたいので、公開されている機械学習モデルの中の「[テキスト生成](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)」を選択します。  
![](https://gyazo.com/d0c01402e78b433ed75145ce47f0615d.png)

画面の右側に利用可能な機械学習モデルのリストが表示されますが、数が非常に多いのでフィルターに「[japanese](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads&search=japanese)」を設定して日本語用でフィルタリングします。  
![](https://gyazo.com/28a1bc9d431b8cad3f01bd3e689b8939.png)

それでも数が多くて迷うのですが、リストの中に表示された「[abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese)」を選択します。  
![](https://gyazo.com/c101e22694eac99439e43c5a92eb22b0.png)

画面右下の「Text Generation」のテキストボックスの中にサンプル文章が表示されています。  
どうやら、このテキストボックスに文字を入力して「Compute」ボタンを押せば、何らかの文章を生成してくれる仕様のようです。  

画面右下の「Text Generation」のテキストボックスに以下の文章を記述して「Compute」ボタンを押してみます。  
`株式会社豆蔵は最先端技術・ノウハウで`  

結果として次のように文章を続けてくれました。  
![](https://gyazo.com/bcc10532b6aca509a067540c7ba56cfd.png)

入力する文章を少し変えてみます。  
`株式会社豆蔵は最先端技術・ノウハウで社会に`  

結果が少し変化しました。  
![](https://gyazo.com/c1b02760325d4f5657ec23bad34f1801.png)

入力に使った文章の元ネタは以下です。  
![](https://gyazo.com/ab6a624b35522dce63d4a23a7103d005.png)

出力された内容はそれなりに筋の通った文章になっていることがわかります。  
ChatGPT へ続く自然言語処理の一端を見れたように思います。

## 今後について

HuggingFaceで想像以上に遊べることがわかったので、今後は
- Python(PyTorchやTensorflow)でプログラムから機械学習モデルを呼び出してみる
- Attensionの可視化をグラフで表現してみる

なども試してみたいと思います。
